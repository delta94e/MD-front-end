# robots.txt â€” Deep Dive!

> **Nguá»“n**: https://nextjs.org/docs/app/api-reference/file-conventions/metadata/robots
> **Spec**: https://en.wikipedia.org/wiki/Robots.txt#Standard
> **NgÃ´n ngá»¯**: Tiáº¿ng Viá»‡t â€” giáº£i thÃ­ch cá»±c ká»³ chi tiáº¿t!
> **Trang nÃ y KHÃ”NG cÃ³ hÃ¬nh/diagram** â€” chá»‰ cÃ³ text + code blocks!
> **Since**: v13.3.0!

---

## Â§1. robots.txt LÃ  GÃ¬?

```
  ROBOTS.TXT â€” Tá»”NG QUAN:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                              â”‚
  â”‚  WHAT:                                                        â”‚
  â”‚  â†’ File cho SEARCH ENGINE CRAWLERS biáº¿t URLs nÃ o              â”‚
  â”‚    Ä‘Æ°á»£c phÃ©p hoáº·c KHÃ”NG Ä‘Æ°á»£c phÃ©p truy cáº­p! â˜…                â”‚
  â”‚  â†’ Theo chuáº©n Robots Exclusion Standard! â˜…                   â”‚
  â”‚  â†’ Äáº·t á»Ÿ ROOT cá»§a app/ directory! â˜…                          â”‚
  â”‚                                                              â”‚
  â”‚  FLOW:                                                        â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
  â”‚  â”‚                                                       â”‚    â”‚
  â”‚  â”‚  Googlebot â”€â”€â†’ GET /robots.txt â”€â”€â†’ Äá»c quy táº¯c!     â”‚    â”‚
  â”‚  â”‚       â”‚                                               â”‚    â”‚
  â”‚  â”‚       â–¼                                               â”‚    â”‚
  â”‚  â”‚  "Allow: /" â†’ OK! Crawl táº¥t cáº£! âœ…                   â”‚    â”‚
  â”‚  â”‚  "Disallow: /private/" â†’ KHÃ”NG crawl! âŒ              â”‚    â”‚
  â”‚  â”‚  "Sitemap: ..." â†’ Biáº¿t cáº¥u trÃºc site! ğŸ“‹            â”‚    â”‚
  â”‚  â”‚                                                       â”‚    â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
  â”‚                                                              â”‚
  â”‚  2 CÃCH Táº O:                                                  â”‚
  â”‚  â†’ CÃ¡ch 1: Static robots.txt file! â˜…                         â”‚
  â”‚  â†’ CÃ¡ch 2: Code generation (robots.ts)! â˜…                   â”‚
  â”‚                                                              â”‚
  â”‚  AI CRAWLER BLOCKING (thá»±c táº¿!):                              â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
  â”‚  â”‚  User-Agent: GPTBot        â†’ Block OpenAI!           â”‚    â”‚
  â”‚  â”‚  User-Agent: ChatGPT-User  â†’ Block ChatGPT Browse!   â”‚    â”‚
  â”‚  â”‚  User-Agent: CCBot         â†’ Block Common Crawl!     â”‚    â”‚
  â”‚  â”‚  User-Agent: anthropic-ai  â†’ Block Claude!           â”‚    â”‚
  â”‚  â”‚  Disallow: /                                          â”‚    â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
  â”‚                                                              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Â§2. CÃ¡ch 1: Static robots.txt!

```
  STATIC FILE:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                              â”‚
  â”‚  FILE: app/robots.txt                                         â”‚
  â”‚                                                              â”‚
  â”‚  User-Agent: *                                                â”‚
  â”‚  Allow: /                                                    â”‚
  â”‚  Disallow: /private/                                         â”‚
  â”‚                                                              â”‚
  â”‚  Sitemap: https://acme.com/sitemap.xml                       â”‚
  â”‚                                                              â”‚
  â”‚  GIáº¢I THÃCH Tá»ªNG DÃ’NG:                                        â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
  â”‚  â”‚ User-Agent: *                                          â”‚    â”‚
  â”‚  â”‚ â†’ Ãp dá»¥ng cho Táº¤T Cáº¢ crawlers! â˜…                      â”‚    â”‚
  â”‚  â”‚ â†’ Google, Bing, Yahoo, Baidu...                        â”‚    â”‚
  â”‚  â”‚                                                       â”‚    â”‚
  â”‚  â”‚ Allow: /                                               â”‚    â”‚
  â”‚  â”‚ â†’ Cho phÃ©p crawl Táº¤T Cáº¢! â˜…                            â”‚    â”‚
  â”‚  â”‚                                                       â”‚    â”‚
  â”‚  â”‚ Disallow: /private/                                    â”‚    â”‚
  â”‚  â”‚ â†’ Cáº¤M crawl /private/ vÃ  sub-paths! â˜…                 â”‚    â”‚
  â”‚  â”‚ â†’ /private/secret âŒ                                   â”‚    â”‚
  â”‚  â”‚ â†’ /private/admin âŒ                                    â”‚    â”‚
  â”‚  â”‚ â†’ /public/ âœ… (KHÃ”NG bá»‹ áº£nh hÆ°á»Ÿng!)                   â”‚    â”‚
  â”‚  â”‚                                                       â”‚    â”‚
  â”‚  â”‚ Sitemap: https://acme.com/sitemap.xml                  â”‚    â”‚
  â”‚  â”‚ â†’ Chá»‰ cho crawler biáº¿t sitemap á»Ÿ Ä‘Ã¢u! â˜…               â”‚    â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
  â”‚                                                              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Â§3. CÃ¡ch 2: Generate Báº±ng Code!

```
  CODE GENERATION:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                              â”‚
  â”‚  FILE: app/robots.ts                                          â”‚
  â”‚                                                              â”‚
  â”‚  import type { MetadataRoute } from 'next'                   â”‚
  â”‚                                                              â”‚
  â”‚  export default function robots(): MetadataRoute.Robots {    â”‚
  â”‚    return {                                                   â”‚
  â”‚      rules: {                                                â”‚
  â”‚        userAgent: '*',                                       â”‚
  â”‚        allow: '/',                                           â”‚
  â”‚        disallow: '/private/',                                â”‚
  â”‚      },                                                      â”‚
  â”‚      sitemap: 'https://acme.com/sitemap.xml',                â”‚
  â”‚    }                                                         â”‚
  â”‚  }                                                           â”‚
  â”‚                                                              â”‚
  â”‚  OUTPUT:                                                      â”‚
  â”‚  User-Agent: *                                                â”‚
  â”‚  Allow: /                                                    â”‚
  â”‚  Disallow: /private/                                         â”‚
  â”‚  Sitemap: https://acme.com/sitemap.xml                       â”‚
  â”‚                                                              â”‚
  â”‚  â˜… "Good to know" tá»« docs:                                    â”‚
  â”‚  â†’ robots.js = Special Route Handler! â˜…                      â”‚
  â”‚  â†’ CACHED by default! â˜…                                      â”‚
  â”‚  â†’ Trá»« khi dÃ¹ng Dynamic API hoáº·c dynamic config! â˜…          â”‚
  â”‚                                                              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  MULTIPLE USER AGENTS:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                              â”‚
  â”‚  export default function robots(): MetadataRoute.Robots {    â”‚
  â”‚    return {                                                   â”‚
  â”‚      rules: [                        â† ARRAY of rules! â˜…    â”‚
  â”‚        {                                                     â”‚
  â”‚          userAgent: 'Googlebot',                             â”‚
  â”‚          allow: ['/'],                                       â”‚
  â”‚          disallow: '/private/',                              â”‚
  â”‚        },                                                    â”‚
  â”‚        {                                                     â”‚
  â”‚          userAgent: ['Applebot', 'Bingbot'],                 â”‚
  â”‚          disallow: ['/'],            â† Block ALL! â˜…          â”‚
  â”‚        },                                                    â”‚
  â”‚      ],                                                      â”‚
  â”‚      sitemap: 'https://acme.com/sitemap.xml',                â”‚
  â”‚    }                                                         â”‚
  â”‚  }                                                           â”‚
  â”‚                                                              â”‚
  â”‚  OUTPUT:                                                      â”‚
  â”‚  User-Agent: Googlebot                                        â”‚
  â”‚  Allow: /                                                    â”‚
  â”‚  Disallow: /private/                                         â”‚
  â”‚                                                              â”‚
  â”‚  User-Agent: Applebot                                         â”‚
  â”‚  Disallow: /                                                 â”‚
  â”‚                                                              â”‚
  â”‚  User-Agent: Bingbot                                          â”‚
  â”‚  Disallow: /                                                 â”‚
  â”‚                                                              â”‚
  â”‚  Sitemap: https://acme.com/sitemap.xml                       â”‚
  â”‚                                                              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Â§4. Robots Object â€” TypeScript Type!

```
  ROBOTS TYPE:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                              â”‚
  â”‚  type Robots = {                                              â”‚
  â”‚    rules:                                                    â”‚
  â”‚      | {                           â† Má»™t rule! â˜…             â”‚
  â”‚          userAgent?: string | string[]                       â”‚
  â”‚          allow?: string | string[]                           â”‚
  â”‚          disallow?: string | string[]                        â”‚
  â”‚          crawlDelay?: number       â† GiÃ¢y giá»¯a requests! â˜…  â”‚
  â”‚        }                                                     â”‚
  â”‚      | Array<{                     â† NHIá»€U rules! â˜…          â”‚
  â”‚          userAgent: string | string[]                        â”‚
  â”‚          allow?: string | string[]                           â”‚
  â”‚          disallow?: string | string[]                        â”‚
  â”‚          crawlDelay?: number                                 â”‚
  â”‚        }>                                                    â”‚
  â”‚    sitemap?: string | string[]     â† 1 hoáº·c NHIá»€U! â˜…        â”‚
  â”‚    host?: string                   â† Domain chÃ­nh! â˜…         â”‚
  â”‚  }                                                           â”‚
  â”‚                                                              â”‚
  â”‚  FIELDS GIáº¢I THÃCH:                                           â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
  â”‚  â”‚ Field        â”‚ MÃ´ táº£                                â”‚     â”‚
  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
  â”‚  â”‚ userAgent    â”‚ Bot nÃ o? "*" = táº¥t cáº£! â˜…              â”‚     â”‚
  â”‚  â”‚ allow        â”‚ ÄÆ°á»ng dáº«n ÄÆ¯á»¢C crawl! â˜…               â”‚     â”‚
  â”‚  â”‚ disallow     â”‚ ÄÆ°á»ng dáº«n Bá»Š Cáº¤M! â˜…                  â”‚     â”‚
  â”‚  â”‚ crawlDelay   â”‚ Delay (giÃ¢y) giá»¯a requests! â˜…        â”‚     â”‚
  â”‚  â”‚ sitemap      â”‚ URL sitemap! â˜…                        â”‚     â”‚
  â”‚  â”‚ host         â”‚ Domain chÃ­nh (mirror sites!) â˜…        â”‚     â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
  â”‚                                                              â”‚
  â”‚  ALLOW vs DISALLOW PRIORITY:                                  â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
  â”‚  â”‚  Disallow: /admin/                                    â”‚    â”‚
  â”‚  â”‚  Allow: /admin/public/                                â”‚    â”‚
  â”‚  â”‚                                                       â”‚    â”‚
  â”‚  â”‚  /admin/secret â†’ âŒ Bá»‹ block!                         â”‚    â”‚
  â”‚  â”‚  /admin/public/page â†’ âœ… ÄÆ°á»£c crawl!                  â”‚    â”‚
  â”‚  â”‚  /admin/public/page/deep â†’ âœ… CÅ©ng Ä‘Æ°á»£c! â˜…            â”‚    â”‚
  â”‚  â”‚                                                       â”‚    â”‚
  â”‚  â”‚  QUY Táº®C: Longest match wins! â˜…                       â”‚    â”‚
  â”‚  â”‚  â†’ Path dÃ i hÆ¡n = Æ°u tiÃªn cao hÆ¡n! â˜…                 â”‚    â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
  â”‚                                                              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Â§5. Tá»± Viáº¿t â€” RobotsEngine!

```javascript
var RobotsEngine = (function () {
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // 1. ROBOTS.TXT GENERATOR
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  function generateRobotsTxt(config) {
    var lines = [];
    var rules = Array.isArray(config.rules) ? config.rules : [config.rules];

    for (var i = 0; i < rules.length; i++) {
      var rule = rules[i];
      var agents = Array.isArray(rule.userAgent)
        ? rule.userAgent
        : [rule.userAgent || "*"];

      for (var a = 0; a < agents.length; a++) {
        lines.push("User-Agent: " + agents[a]);

        // Allow
        if (rule.allow) {
          var allows = Array.isArray(rule.allow) ? rule.allow : [rule.allow];
          for (var j = 0; j < allows.length; j++) {
            lines.push("Allow: " + allows[j]);
          }
        }

        // Disallow
        if (rule.disallow) {
          var disallows = Array.isArray(rule.disallow)
            ? rule.disallow
            : [rule.disallow];
          for (var k = 0; k < disallows.length; k++) {
            lines.push("Disallow: " + disallows[k]);
          }
        }

        // Crawl-delay
        if (rule.crawlDelay) {
          lines.push("Crawl-delay: " + rule.crawlDelay);
        }

        lines.push(""); // blank line between agents
      }
    }

    // Sitemap(s)
    if (config.sitemap) {
      var sitemaps = Array.isArray(config.sitemap)
        ? config.sitemap
        : [config.sitemap];
      for (var s = 0; s < sitemaps.length; s++) {
        lines.push("Sitemap: " + sitemaps[s]);
      }
    }

    // Host
    if (config.host) {
      lines.push("Host: " + config.host);
    }

    return lines.join("\n");
  }

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // 2. CRAWL CHECKER
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  function canCrawl(rules, userAgent, path) {
    var rulesList = Array.isArray(rules) ? rules : [rules];
    var matched = null;

    // Find matching rule for userAgent
    for (var i = 0; i < rulesList.length; i++) {
      var r = rulesList[i];
      var agents = Array.isArray(r.userAgent) ? r.userAgent : [r.userAgent];
      for (var a = 0; a < agents.length; a++) {
        if (agents[a] === "*" || agents[a] === userAgent) {
          matched = r;
          break;
        }
      }
      if (matched) break;
    }

    if (!matched)
      return { allowed: true, reason: "KhÃ´ng cÃ³ rule â†’ máº·c Ä‘á»‹nh ALLOW! â˜…" };

    // Longest match wins!
    var bestAllow = "",
      bestDisallow = "";
    if (matched.allow) {
      var allows = Array.isArray(matched.allow)
        ? matched.allow
        : [matched.allow];
      for (var j = 0; j < allows.length; j++) {
        if (
          path.indexOf(allows[j]) === 0 &&
          allows[j].length > bestAllow.length
        ) {
          bestAllow = allows[j];
        }
      }
    }
    if (matched.disallow) {
      var disallows = Array.isArray(matched.disallow)
        ? matched.disallow
        : [matched.disallow];
      for (var k = 0; k < disallows.length; k++) {
        if (
          path.indexOf(disallows[k]) === 0 &&
          disallows[k].length > bestDisallow.length
        ) {
          bestDisallow = disallows[k];
        }
      }
    }

    if (bestAllow.length > bestDisallow.length) {
      return {
        allowed: true,
        reason: "Allow '" + bestAllow + "' dÃ i hÆ¡n â†’ ALLOW! â˜…",
      };
    }
    if (bestDisallow.length > 0) {
      return {
        allowed: false,
        reason: "Disallow '" + bestDisallow + "' match â†’ BLOCK! âŒ",
      };
    }
    return { allowed: true, reason: "KhÃ´ng match disallow â†’ ALLOW! â˜…" };
  }

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // 3. DEMO
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  function demo() {
    console.log("â•â•â• Robots Engine â•â•â•");

    console.log("\nâ”€â”€ Generate â”€â”€");
    var txt = generateRobotsTxt({
      rules: [
        { userAgent: "Googlebot", allow: ["/"], disallow: "/private/" },
        { userAgent: ["Applebot", "Bingbot"], disallow: ["/"] },
        { userAgent: "GPTBot", disallow: ["/"], crawlDelay: 10 },
      ],
      sitemap: [
        "https://acme.com/sitemap.xml",
        "https://acme.com/sitemap-blog.xml",
      ],
      host: "https://acme.com",
    });
    console.log(txt);

    console.log("\nâ”€â”€ Crawl Check â”€â”€");
    var rules = [
      {
        userAgent: "Googlebot",
        allow: ["/", "/admin/public/"],
        disallow: "/admin/",
      },
      { userAgent: "GPTBot", disallow: ["/"] },
    ];
    console.log("/blog Googlebot:", canCrawl(rules, "Googlebot", "/blog"));
    console.log(
      "/admin/secret Googlebot:",
      canCrawl(rules, "Googlebot", "/admin/secret"),
    );
    console.log(
      "/admin/public/page:",
      canCrawl(rules, "Googlebot", "/admin/public/page"),
    );
    console.log("/ GPTBot:", canCrawl(rules, "GPTBot", "/"));
    console.log("/ YandexBot:", canCrawl(rules, "YandexBot", "/"));
  }

  return { demo: demo };
})();
// Cháº¡y: RobotsEngine.demo();
```

---

## Â§6. CÃ¢u Há»i Phá»ng Váº¥n!

```
  CÃ‚U Há»I:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                              â”‚
  â”‚  â“ 1: robots.txt dÃ¹ng Ä‘á»ƒ lÃ m gÃ¬?                              â”‚
  â”‚  â†’ Cho search engine crawlers biáº¿t URLs nÃ o Ä‘Æ°á»£c crawl! â˜…   â”‚
  â”‚  â†’ Theo Robots Exclusion Standard! â˜…                         â”‚
  â”‚  â†’ KHÃ”NG báº£o máº­t! Chá»‰ lÃ  "yÃªu cáº§u lá»‹ch sá»±"! â˜…              â”‚
  â”‚  â†’ Bad bots cÃ³ thá»ƒ IGNORE! â†’ Cáº§n auth/middleware thá»±c sá»±!   â”‚
  â”‚                                                              â”‚
  â”‚  â“ 2: Static (.txt) vs Code (.ts) â€” khÃ¡c gÃ¬?                  â”‚
  â”‚  â†’ Static: cá»‘ Ä‘á»‹nh, Ä‘Æ¡n giáº£n, khÃ´ng logic!                  â”‚
  â”‚  â†’ Code: dynamic values, env-based, TYPE SAFE! â˜…            â”‚
  â”‚  â†’ Code: MetadataRoute.Robots type! â˜…                       â”‚
  â”‚  â†’ Code: Special Route Handler, CACHED by default! â˜…        â”‚
  â”‚                                                              â”‚
  â”‚  â“ 3: crawlDelay lÃ  gÃ¬?                                       â”‚
  â”‚  â†’ YÃªu cáº§u bot Ä‘á»£i N giÃ¢y giá»¯a má»—i request! â˜…              â”‚
  â”‚  â†’ Giáº£m táº£i server! â˜…                                       â”‚
  â”‚  â†’ Google KHÃ”NG tÃ´n trá»ng crawlDelay! â˜…                     â”‚
  â”‚  â†’ Bing, Yahoo, Yandex CÃ“ tÃ´n trá»ng! â˜…                     â”‚
  â”‚                                                              â”‚
  â”‚  â“ 4: Allow vs Disallow â€” cÃ¡i nÃ o Æ°u tiÃªn?                    â”‚
  â”‚  â†’ Longest match wins! â˜…â˜…â˜…                                   â”‚
  â”‚  â†’ Disallow: /admin/ + Allow: /admin/public/                 â”‚
  â”‚  â†’ /admin/secret â†’ âŒ (match /admin/ ngáº¯n hÆ¡n!)             â”‚
  â”‚  â†’ /admin/public/x â†’ âœ… (match /admin/public/ dÃ i hÆ¡n!)     â”‚
  â”‚                                                              â”‚
  â”‚  â“ 5: robots.txt cÃ³ cháº·n Ä‘Æ°á»£c AI crawlers khÃ´ng?              â”‚
  â”‚  â†’ CÃ“, náº¿u AI bot TÃ”N TRá»ŒNG quy táº¯c! â˜…                    â”‚
  â”‚  â†’ GPTBot (OpenAI), CCBot, anthropic-ai! â˜…                  â”‚
  â”‚  â†’ NhÆ°ng KHÃ”NG báº£o Ä‘áº£m! Cáº§n thÃªm biá»‡n phÃ¡p khÃ¡c! â˜…         â”‚
  â”‚                                                              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
