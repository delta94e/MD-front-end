# Large File Upload & Resume â€” Deep Dive

> ğŸ“… 2026-02-12 Â· â± 20 phÃºt Ä‘á»c
>
> ByteDance Interview: Implement large file upload with chunking,
> resume upload, instant upload, progress tracking.
> Frontend: Vue + Element-UI | Server: Node.js + multiparty
> Äá»™ khÃ³: â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ | Chá»§ Ä‘á»: File Upload / Blob / Stream / Web Worker

---

## Má»¥c Lá»¥c

0. [Tá»•ng Quan Architecture](#tá»•ng-quan)
1. [Blob.prototype.slice â€” File Chunking](#chunking)
2. [Frontend â€” Upload Chunks](#frontend-upload)
3. [Server â€” Receive & Merge](#server-merge)
4. [Progress Bar â€” Chunk + Overall](#progress)
5. [Hash Generation â€” Web Worker + Spark-MD5](#hash)
6. [Instant Upload â€” ç§’ä¼ ](#instant-upload)
7. [Pause Upload â€” XHR Abort](#pause)
8. [Resume Upload â€” æ–­ç‚¹ç»­ä¼ ](#resume)
9. [Progress Bar Fix â€” Fake Progress](#progress-fix)
10. [Production Considerations](#production)
11. [TÃ³m Táº¯t & Checklist](#tÃ³m-táº¯t)

---

## Tá»•ng Quan

```
LARGE FILE UPLOAD â€” OVERALL ARCHITECTURE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                     FRONTEND (Vue)                      â”‚
  â”‚                                                         â”‚
  â”‚  â‘  File Input â†’ Blob.slice() â†’ chunks[]                â”‚
  â”‚  â‘¡ Web Worker â†’ SparkMD5 â†’ fileHash                    â”‚
  â”‚  â‘¢ Verify API â†’ check: instant upload? resume?         â”‚
  â”‚  â‘£ FormData + XHR â†’ concurrent upload chunks           â”‚
  â”‚  â‘¤ Merge request â†’ notify server to merge              â”‚
  â”‚  â‘¥ Progress: xhr.upload.onprogress per chunk           â”‚
  â”‚  â‘¦ Pause: xhr.abort() | Resume: skip uploaded chunks   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ HTTP
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                   SERVER (Node.js)                      â”‚
  â”‚                                                         â”‚
  â”‚  â‘  multiparty â†’ parse FormData â†’ save chunk to disk    â”‚
  â”‚  â‘¡ /verify â†’ check file exists? return uploaded chunks  â”‚
  â”‚  â‘¢ /merge  â†’ readStream.pipe(writeStream) at position  â”‚
  â”‚             â†’ concurrent merge â†’ delete chunks folder   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  FLOW:
  File â†’ slice() â†’ [chunk-0, chunk-1, ..., chunk-N]
                         â†“ concurrent upload (Promise.all)
  Server:  chunkDir/hash-0, hash-1, ..., hash-N
                         â†“ merge request
  Server:  final-file.ext (readStream â†’ writeStream at offset)
```

```
FEATURES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Large Upload     â”‚ Blob.slice â†’ concurrent chunks      â”‚
  â”‚ Progress         â”‚ xhr.upload.onprogress per chunk     â”‚
  â”‚ Hash             â”‚ Web Worker + SparkMD5 (non-blocking)â”‚
  â”‚ Instant Upload   â”‚ Hash match â†’ skip upload entirely   â”‚
  â”‚ Pause            â”‚ xhr.abort() on all active XHRs      â”‚
  â”‚ Resume           â”‚ Server returns uploaded chunks list â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Â§1. Blob.prototype.slice â€” File Chunking

```
FILE CHUNKING PRINCIPLE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Blob.prototype.slice(start, end) â†’ new Blob (sub-slice)

  100MB File, SIZE = 10MB:
  â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
  â”‚ 0-10 â”‚10-20 â”‚20-30 â”‚30-40 â”‚40-50 â”‚50-60 â”‚60-70 â”‚70-80 â”‚80-90 â”‚90-100â”‚
  â””â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”˜
     â†“      â†“      â†“      â†“      â†“      â†“      â†“      â†“      â†“      â†“
  chunk-0 chunk-1 chunk-2  ...                                    chunk-9

  â†’ File giá»‘ng Array, slice() giá»‘ng Array.slice()
  â†’ Concurrent upload: biáº¿n 1 file lá»›n â†’ N file nhá» song song
  â†’ Giáº£m thá»i gian upload ÄÃNG Ká»‚!
```

```javascript
const SIZE = 10 * 1024 * 1024; // 10MB per chunk

function createFileChunk(file, size = SIZE) {
  const fileChunkList = [];
  let cur = 0;
  while (cur < file.size) {
    fileChunkList.push({
      file: file.slice(cur, cur + size), // Blob.slice!
    });
    cur += size;
  }
  return fileChunkList;
}
// 100MB file â†’ 10 chunks Ã— 10MB
```

---

## Â§2. Frontend â€” Upload Chunks

### XHR Request Wrapper

```javascript
// Wrapper XHR â€” khÃ´ng dÃ¹ng library, phá»ng váº¥n thÆ°á»ng yÃªu cáº§u native
function request({
  url,
  method = "post",
  data,
  headers = {},
  onProgress = (e) => e, // â† Progress callback
  requestList, // â† Track XHR for pause/abort
}) {
  return new Promise((resolve) => {
    const xhr = new XMLHttpRequest();

    // â‘  Progress listener
    xhr.upload.onprogress = onProgress;

    xhr.open(method, url);

    // â‘¡ Set custom headers
    Object.keys(headers).forEach((key) =>
      xhr.setRequestHeader(key, headers[key]),
    );

    xhr.send(data);

    xhr.onload = (e) => {
      // â‘¢ Upload done â†’ remove from requestList
      if (requestList) {
        const idx = requestList.findIndex((item) => item === xhr);
        requestList.splice(idx, 1);
      }
      resolve({ data: e.target.response });
    };

    // â‘£ Expose XHR for pause/abort
    requestList?.push(xhr);
  });
}
```

### Upload Flow

```javascript
// Step 1: File change â†’ store file
handleFileChange(e) {
    const [file] = e.target.files;
    if (!file) return;
    this.container.file = file;
}

// Step 2: Upload button clicked
async handleUpload() {
    if (!this.container.file) return;

    // â‘  Táº¡o chunks
    const fileChunkList = this.createFileChunk(this.container.file);

    // â‘¡ TÃ­nh hash (Web Worker â€” xem Â§5)
    this.container.hash = await this.calculateHash(fileChunkList);

    // â‘¢ Verify: instant upload? resume?
    const { shouldUpload, uploadedList } = await this.verifyUpload(
        this.container.file.name,
        this.container.hash
    );

    // â‘£ Instant upload: file Ä‘Ã£ tá»“n táº¡i trÃªn server
    if (!shouldUpload) {
        this.$message.success("ç§’ä¼ : file upload success!");
        return;
    }

    // â‘¤ Chuáº©n bá»‹ data cho má»—i chunk
    this.data = fileChunkList.map(({ file }, index) => ({
        fileHash: this.container.hash,
        index,
        hash: this.container.hash + "-" + index,  // unique chunk ID
        chunk: file,
        percentage: uploadedList.includes(index) ? 100 : 0
    }));

    // â‘¥ Upload!
    await this.uploadChunks(uploadedList);
}
```

### Upload Chunks + Merge

```javascript
// Upload chunks, filter Ä‘Ã£ uploaded (resume)
async uploadChunks(uploadedList = []) {
    const requestList = this.data
        // â‘  Filter: skip Ä‘Ã£ uploaded chunks (resume!)
        .filter(({ hash }) => !uploadedList.includes(hash))
        .map(({ chunk, hash, index }) => {
            const formData = new FormData();
            formData.append("chunk", chunk);       // Blob data
            formData.append("hash", hash);          // chunk ID
            formData.append("filename", this.container.file.name);
            formData.append("fileHash", this.container.hash);
            return { formData, index };
        })
        .map(({ formData, index }) =>
            this.request({
                url: "http://localhost:3000",
                data: formData,
                onProgress: this.createProgressHandler(this.data[index]),
                requestList: this.requestList  // track for abort
            })
        );

    // â‘¡ Concurrent upload â€” Promise.all!
    await Promise.all(requestList);

    // â‘¢ All chunks done â†’ merge request
    if (uploadedList.length + requestList.length === this.data.length) {
        await this.mergeRequest();
    }
}

// Merge request â€” notify server
async mergeRequest() {
    await this.request({
        url: "http://localhost:3000/merge",
        headers: { "content-type": "application/json" },
        data: JSON.stringify({
            size: SIZE,                          // chunk size for offset
            filename: this.container.file.name
        })
    });
}
```

```
Táº I SAO DÃ™NG Promise.all?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Sequential upload:      â”€â”€[chunk0]â”€â”€[chunk1]â”€â”€[chunk2]â”€â”€â†’ SLOW
  Concurrent (Promise.all): â”€â”€[chunk0]â”€â”€â†’
                            â”€â”€[chunk1]â”€â”€â†’  ALL parallel â†’ FAST!
                            â”€â”€[chunk2]â”€â”€â†’

  âš ï¸ Concurrent = thá»© tá»± nháº­n KHÃ”NG Ä‘áº£m báº£o
  â†’ Cáº§n hash index Ä‘á»ƒ server merge ÄÃšNG thá»© tá»±!
```

---

## Â§3. Server â€” Receive & Merge

### Receive Chunks

```javascript
const http = require("http");
const path = require("path");
const fse = require("fs-extra");
const multiparty = require("multiparty");

const UPLOAD_DIR = path.resolve(__dirname, "..", "target");

server.on("request", async (req, res) => {
  res.setHeader("Access-Control-Allow-Origin", "*");
  res.setHeader("Access-Control-Allow-Headers", "*");

  if (req.method === "OPTIONS") {
    res.status = 200;
    res.end();
    return;
  }

  // â‘  Parse FormData
  const multipart = new multiparty.Form();

  multipart.parse(req, async (err, fields, files) => {
    if (err) return;

    const [chunk] = files.chunk; // file object
    const [hash] = fields.hash; // "hash-0"
    const [filename] = fields.filename;
    const [fileHash] = fields.fileHash;

    // â‘¡ Táº¡o folder táº¡m: chunkDir_<fileHash>
    const chunkDir = path.resolve(UPLOAD_DIR, "chunkDir_" + fileHash);
    if (!fse.existsSync(chunkDir)) {
      await fse.mkdirs(chunkDir);
    }

    // â‘¢ Move chunk tá»« temp â†’ chunkDir/<hash>
    await fse.move(chunk.path, `${chunkDir}/${hash}`);

    res.end("received file chunk");
  });
});
```

```
SERVER FOLDER STRUCTURE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  target/
  â””â”€â”€ chunkDir_274740166ba5b3948d17fd71c06cb918/
      â”œâ”€â”€ 274740166ba5b3948d17fd71c06cb918-0
      â”œâ”€â”€ 274740166ba5b3948d17fd71c06cb918-1
      â”œâ”€â”€ 274740166ba5b3948d17fd71c06cb918-2
      â”œâ”€â”€ ...
      â””â”€â”€ 274740166ba5b3948d17fd71c06cb918-11

  â†’ Má»—i file = 1 chunk
  â†’ TÃªn = fileHash + "-" + index
  â†’ Folder prefix: "chunkDir_" + fileHash
```

### Merge Chunks

```javascript
const extractExt = (filename) =>
  filename.slice(filename.lastIndexOf("."), filename.length);

// Pipe stream: readStream â†’ writeStream
const pipeStream = (path, writeStream) =>
  new Promise((resolve) => {
    const readStream = fse.createReadStream(path);
    readStream.on("end", () => {
      fse.unlinkSync(path); // XÃ³a chunk sau khi merge
      resolve();
    });
    readStream.pipe(writeStream);
  });

// Merge táº¥t cáº£ chunks â†’ final file
const mergeFileChunk = async (filePath, fileHash, size) => {
  const chunkDir = path.resolve(UPLOAD_DIR, "chunkDir_" + fileHash);
  const chunkPaths = await fse.readdir(chunkDir);

  // â‘  Sort theo index (filesystem order â‰  upload order!)
  chunkPaths.sort((a, b) => a.split("-")[1] - b.split("-")[1]);

  // â‘¡ Concurrent merge: má»—i chunk â†’ write táº¡i ÄÃšNG position
  await Promise.all(
    chunkPaths.map((chunkPath, index) =>
      pipeStream(
        path.resolve(chunkDir, chunkPath),
        fse.createWriteStream(filePath, {
          start: index * size, // â† KEY: offset position!
        }),
      ),
    ),
  );

  // â‘¢ XÃ³a folder chunks sau khi merge xong
  fse.rmdirSync(chunkDir);
};
```

```
MERGE STRATEGY â€” createWriteStream({ start }):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  File 50MB, chunk size = 10MB:

  WriteStream positions:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ start: 0     â”‚ start: 10MB  â”‚ start: 20MB  â”‚ start: 30MB â”‚ start: 40MB â”‚
  â”‚ â† chunk-0 â†’ â”‚ â† chunk-1 â†’ â”‚ â† chunk-2 â†’ â”‚ â† chunk-3 â†’â”‚ â† chunk-4 â†’â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â†’ Má»—i readStream pipe vÃ o ÄÃšNG vá»‹ trÃ­ trong writeStream
  â†’ Concurrent merge: stream order KHÃ”NG quan trá»ng!
  â†’ start = index Ã— chunkSize

  âš ï¸ Alternative: sequential merge (Ä‘á»£i chunk trÆ°á»›c xong má»›i merge tiáº¿p)
  â†’ KHÃ”NG cáº§n start position
  â†’ NhÆ°ng CHáº¬M hÆ¡n!
  â†’ â†’ NÃªn dÃ¹ng concurrent + start position â­
```

---

## Â§4. Progress Bar â€” Chunk + Overall

### Single Chunk Progress

```javascript
// Factory function: má»—i chunk â†’ riÃªng listener
createProgressHandler(item) {
    return e => {
        item.percentage = parseInt(
            String((e.loaded / e.total) * 100)
        );
    };
}

// Sá»­ dá»¥ng:
this.request({
    url: "http://localhost:3000",
    data: formData,
    onProgress: this.createProgressHandler(this.data[index])
});
```

```
XHR PROGRESS EVENT:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  xhr.upload.onprogress = function(e) {
      e.loaded  â†’ bytes Ä‘Ã£ upload
      e.total   â†’ tá»•ng bytes
      e.loaded / e.total Ã— 100 = percentage
  };

  â†’ NATIVE XHR support! KhÃ´ng cáº§n library
  â†’ Má»—i chunk cÃ³ riÃªng onprogress â†’ track riÃªng percentage
```

### Overall Progress (Computed Property)

```javascript
computed: {
    uploadPercentage() {
        if (!this.container.file || !this.data.length) return 0;
        const loaded = this.data
            .map(item => item.size * item.percentage)
            .reduce((acc, cur) => acc + cur);
        return parseInt(
            (loaded / this.container.file.size).toFixed(2)
        );
    }
}

// Overall = Î£(chunk.size Ã— chunk.percentage) / file.size
// â†’ Weighted average theo kÃ­ch thÆ°á»›c chunk
```

---

## Â§5. Hash Generation â€” Web Worker + Spark-MD5

```
Táº I SAO Cáº¦N CONTENT HASH?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  TRÆ¯á»šC: hash = filename + "-" + index
  â†’ Äá»•i tÃªn file â†’ hash thay Ä‘á»•i â†’ KHÃ”NG nháº­n láº¡i uploaded chunks!

  SAU: hash = MD5(file content)
  â†’ File content KHÃ”NG Ä‘á»•i â†’ hash KHÃ”NG Ä‘á»•i â†’ resume works!
  â†’ Giá»‘ng Webpack contenthash concept!

  Táº I SAO WEB WORKER?
  â†’ File lá»›n â†’ Ä‘á»c + tÃ­nh hash Ráº¤T CHáº¬M
  â†’ Main thread bá»‹ block â†’ UI FREEZE!
  â†’ Web Worker = background thread â†’ UI váº«n responsive â­
```

### Web Worker â€” hash.js

```javascript
// /public/hash.js â€” Worker thread
self.importScripts("/spark-md5.min.js"); // CDN import trong worker

self.onmessage = (e) => {
  const { fileChunkList } = e.data;
  const spark = new self.SparkMD5.ArrayBuffer();
  let percentage = 0;
  let count = 0;

  const loadNext = (index) => {
    const reader = new FileReader();
    reader.readAsArrayBuffer(fileChunkList[index].file);

    reader.onload = (e) => {
      count++;
      spark.append(e.target.result); // Feed chunk vÃ o SparkMD5

      if (count === fileChunkList.length) {
        // â‘  Xong táº¥t cáº£ â†’ gá»­i hash vá» main thread
        self.postMessage({
          percentage: 100,
          hash: spark.end(), // Final MD5 hash
        });
        self.close();
      } else {
        // â‘¡ ChÆ°a xong â†’ report progress
        percentage += 100 / fileChunkList.length;
        self.postMessage({ percentage });
        loadNext(count); // Recursive next chunk
      }
    };
  };

  loadNext(0);
};
```

### Main Thread Communication

```javascript
calculateHash(fileChunkList) {
    return new Promise(resolve => {
        this.container.worker = new Worker("/hash.js");

        // â‘  Gá»­i chunks sang worker
        this.container.worker.postMessage({ fileChunkList });

        // â‘¡ Nháº­n káº¿t quáº£ tá»« worker
        this.container.worker.onmessage = e => {
            const { percentage, hash } = e.data;
            this.hashPercentage = percentage;  // Progress bar
            if (hash) {
                resolve(hash);  // Done!
            }
        };
    });
}
```

```
HASH FLOW:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Main Thread                    Worker Thread (hash.js)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  postMessage({fileChunkList})
           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’         onmessage: receive chunks
                                 FileReader.readAsArrayBuffer(chunk0)
           â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         postMessage({percentage: 10%})
                                 FileReader.readAsArrayBuffer(chunk1)
           â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         postMessage({percentage: 20%})
                                 ...
           â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         postMessage({percentage: 100, hash: "abc123"})
  resolve(hash)                  self.close()

  âš ï¸ SparkMD5: PHáº¢I pass tá»«ng chunk, KHÃ”NG pass cáº£ file!
  â†’ Náº¿u pass cáº£ file â†’ files khÃ¡c nhau cÃ³ thá»ƒ cÃ¹ng hash!
```

---

## Â§6. Instant Upload â€” ç§’ä¼ 

```
INSTANT UPLOAD â€” "ç§’ä¼ ":
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  "ç§’ä¼ " = file ÄÃƒ Tá»’N Táº I trÃªn server â†’ skip upload!

  Flow:
  â‘  Frontend: tÃ­nh hash â†’ gá»­i hash + filename lÃªn /verify
  â‘¡ Server: check file hash tá»“n táº¡i?
     â†’ YES: { shouldUpload: false }
     â†’ NO:  { shouldUpload: true, uploadedList: [...] }
  â‘¢ Frontend:
     â†’ shouldUpload = false â†’ "Upload thÃ nh cÃ´ng!" (instant âš¡)
     â†’ shouldUpload = true  â†’ báº¯t Ä‘áº§u upload

  Thá»±c cháº¥t KHÃ”NG upload gÃ¬ cáº£ â€” chá»‰ lÃ  UI trick!
  â†’ User tháº¥y "upload xong" 1 giÃ¢y, thá»±c ra file Ä‘Ã£ cÃ³ sáºµn :)
```

### Server Verify Endpoint

```javascript
const extractExt = (filename) =>
  filename.slice(filename.lastIndexOf("."), filename.length);

// Tráº£ vá» danh sÃ¡ch chunks Ä‘Ã£ uploaded (cho resume)
const createUploadedList = async (fileHash) =>
  fse.existsSync(path.resolve(UPLOAD_DIR, fileHash))
    ? await fse.readdir(path.resolve(UPLOAD_DIR, fileHash))
    : [];

// /verify endpoint
if (req.url === "/verify") {
  const data = await resolvePost(req);
  const { fileHash, filename } = data;
  const ext = extractExt(filename);
  const filePath = path.resolve(UPLOAD_DIR, `${fileHash}${ext}`);

  if (fse.existsSync(filePath)) {
    // â‘  File Ä‘Ã£ tá»“n táº¡i â†’ instant upload!
    res.end(JSON.stringify({ shouldUpload: false }));
  } else {
    // â‘¡ File chÆ°a cÃ³ â†’ upload, kÃ¨m danh sÃ¡ch Ä‘Ã£ uploaded
    res.end(
      JSON.stringify({
        shouldUpload: true,
        uploadedList: await createUploadedList(fileHash),
      }),
    );
  }
}
```

---

## Â§7. Pause Upload â€” XHR Abort

```
PAUSE PRINCIPLE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  XMLHttpRequest.abort() â†’ cancel request!

  CÃ¡ch track XHR:
  â‘  Má»—i request() â†’ push xhr vÃ o requestList[]
  â‘¡ Upload xong â†’ splice xhr ra khá»i requestList[]
  â†’ requestList chá»‰ chá»©a ÄANG upload XHRs

  Pause:
  requestList.forEach(xhr => xhr?.abort())
  requestList = []

  â†’ Táº¥t cáº£ Ä‘ang upload bá»‹ cancel!
  â†’ Browser console: "net::ERR_ABORTED"
```

```javascript
// Pause button handler
handlePause() {
    this.requestList.forEach(xhr => xhr?.abort());
    this.requestList = [];
}

// requestList tracking trong request():
// â†’ push khi táº¡o XHR
// â†’ splice khi upload xong (onload)
// â†’ abort khi pause
```

---

## Â§8. Resume Upload â€” æ–­ç‚¹ç»­ä¼ 

```
RESUME UPLOAD â€” 2 APPROACHES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â‘  Frontend localStorage (lÆ°u hash Ä‘Ã£ upload)
     â†’ âŒ Máº¥t khi Ä‘á»•i browser!

  â‘¡ Server-side storage (server lÆ°u chunks Ä‘Ã£ nháº­n) â­
     â†’ âœ… Browser-independent!
     â†’ Server tráº£ vá» uploadedList â†’ frontend skip

  FLOW:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      /verify          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Frontend â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   â”‚  Server  â”‚
  â”‚          â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚          â”‚
  â”‚          â”‚   {uploadedList:      â”‚ readdir  â”‚
  â”‚          â”‚    ["hash-0","hash-1" â”‚ chunkDir â”‚
  â”‚          â”‚     "hash-2"]}        â”‚          â”‚
  â”‚          â”‚                       â”‚          â”‚
  â”‚ filter:  â”‚                       â”‚          â”‚
  â”‚ skip 0,1,â”‚                       â”‚          â”‚
  â”‚ 2        â”‚                       â”‚          â”‚
  â”‚          â”‚  upload hash-3,4,...   â”‚          â”‚
  â”‚          â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   â”‚          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```javascript
// Resume button handler
async handleResume() {
    // â‘  Verify â†’ get uploadedList
    const { uploadedList } = await this.verifyUpload(
        this.container.file.name,
        this.container.hash
    );
    // â‘¡ Upload only remaining chunks
    await this.uploadChunks(uploadedList);
}

// uploadChunks Ä‘Ã£ cÃ³ filter logic:
// .filter(({ hash }) => !uploadedList.includes(hash))
// â†’ Chá»‰ upload chunks CHÆ¯A cÃ³ trÃªn server!
```

```
COMPLETE RESUME FLOW:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â‘  User chá»n file â†’ createFileChunk â†’ 10 chunks
  â‘¡ calculateHash â†’ "abc123" (Web Worker)
  â‘¢ verifyUpload("video.mp4", "abc123")
     â†’ Server: file chÆ°a cÃ³, uploadedList = []
  â‘£ Upload 10 chunks concurrent...
  â‘¤ Upload Ä‘áº¿n chunk-5 â†’ User click PAUSE
     â†’ xhr.abort() Ã— 4 (chunks 6-9 bá»‹ cancel)
     â†’ Server Ä‘Ã£ nháº­n: chunk-0 â†’ chunk-5
  â‘¥ User click RESUME
  â‘¦ verifyUpload("video.mp4", "abc123")
     â†’ Server: uploadedList = ["abc123-0"..."abc123-5"]
  â‘§ Frontend filter: skip 0-5, upload 6-9 only!
  â‘¨ All done â†’ mergeRequest()
  â‘© Server merge â†’ final file âœ…
```

---

## Â§9. Progress Bar Fix â€” Fake Progress

```
PROBLEM â€” PROGRESS BAR REGRESSION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Pause â†’ abort XHR â†’ chunk progress RESET to 0
  Resume â†’ new XHR â†’ progress starts from 0 again
  â†’ Overall progress bar ÄI LÃ™I! (60% â†’ 30%) ğŸ˜±

  SOLUTION â€” "FAKE" PROGRESS BAR:
  â†’ Track fakeUploadPercentage
  â†’ Chá»‰ TÄ‚NG, KHÃ”NG BAO GIá»œ GIáº¢M!
  â†’ Khi real progress > fake â†’ fake = real
  â†’ Khi real progress < fake â†’ fake Äá»¨NG YÃŠN (no regression!)
```

```javascript
data: () => ({
    fakeUploadPercentage: 0
}),

computed: {
    // Real progress (cÃ³ thá»ƒ giáº£m khi resume)
    uploadPercentage() {
        if (!this.container.file || !this.data.length) return 0;
        const loaded = this.data
            .map(item => item.size * item.percentage)
            .reduce((acc, cur) => acc + cur);
        return parseInt((loaded / this.container.file.size).toFixed(2));
    }
},

watch: {
    // Fake progress: CHá»ˆ TÄ‚NG!
    uploadPercentage(now) {
        if (now > this.fakeUploadPercentage) {
            this.fakeUploadPercentage = now;  // TÄƒng
        }
        // now < fake â†’ KHÃ”NG lÃ m gÃ¬ â†’ progress Ä‘á»©ng yÃªn!
    }
}

// Hiá»ƒn thá»‹ fakeUploadPercentage cho user, KHÃ”NG pháº£i uploadPercentage!
```

---

## Â§10. Production Considerations

```
PRODUCTION â€” NHá»®NG Váº¤N Äá»€ Cáº¦N Xá»¬ LÃ:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â‘  CHUNK UPLOAD FAILURE HANDLING:
  â†’ Retry failed chunks (exponential backoff)
  â†’ Set max retry count
  â†’ Report failed chunks to user

  â‘¡ CONCURRENCY LIMIT:
  â†’ Promise.all upload Táº¤T Cáº¢ chunks = quÃ¡ nhiá»u connections!
  â†’ NÃªn giá»›i háº¡n: max 3-6 concurrent uploads
  â†’ DÃ¹ng Promise Pool / p-limit library

  â‘¢ HASH OPTIMIZATION:
  â†’ File lá»›n (>1GB) â†’ hash tÃ­nh Ráº¤T LÃ‚U
  â†’ Sampling strategy: chá»‰ hash Ä‘áº§u/cuá»‘i + random middle chunks
  â†’ Trade-off: speed vs collision risk

  â‘£ CHUNK SIZE OPTIMIZATION:
  â†’ QuÃ¡ nhá»: quÃ¡ nhiá»u HTTP requests â†’ overhead
  â†’ QuÃ¡ lá»›n: 1 chunk fail â†’ pháº£i upload láº¡i nhiá»u
  â†’ Sweet spot: 5-10MB (tÃ¹y network)
  â†’ Dynamic sizing: based on connection speed

  â‘¤ SERVER-SIDE CLEANUP:
  â†’ Orphan chunks (user never completed upload)
  â†’ Scheduled cleanup job (delete old chunkDir)
  â†’ TTL cho chunk folders

  â‘¥ SECURITY:
  â†’ File type validation (server-side!)
  â†’ File size limit
  â†’ Rate limiting
  â†’ Virus scanning after merge
```

```
CONCURRENCY LIMIT â€” PRODUCTION VERSION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  // Thay Promise.all â†’ concurrent pool
  async function uploadWithLimit(tasks, limit = 3) {
      const pool = new Set();
      const results = [];

      for (const task of tasks) {
          const p = task().then(res => {
              pool.delete(p);
              return res;
          });
          pool.add(p);
          results.push(p);

          if (pool.size >= limit) {
              await Promise.race(pool);  // Wait cho 1 slot trá»‘ng
          }
      }

      return Promise.all(results);
  }

  â†’ Max 3 uploads cÃ¹ng lÃºc
  â†’ Slot trá»‘ng â†’ cháº¡y task tiáº¿p
  â†’ KHÃ”NG overwhelm server/browser connections
```

```
HASH OPTIMIZATION â€” SAMPLING:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Thay vÃ¬ hash TOÃ€N Bá»˜ file:

  â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚      â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚      â”‚      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
  â”‚ FULL â”‚      â”‚      â”‚SAMPLEâ”‚      â”‚      â”‚      â”‚ FULL â”‚
  â”‚first â”‚ skip â”‚ skip â”‚middleâ”‚ skip â”‚ skip â”‚ skip â”‚ last â”‚
  â”‚2MB   â”‚      â”‚      â”‚2MB   â”‚      â”‚      â”‚      â”‚2MB   â”‚
  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

  â†’ Hash first 2MB + middle samples + last 2MB
  â†’ 1GB file: thay vÃ¬ hash 1GB â†’ chá»‰ hash ~6MB!
  â†’ Risk: collision (2 files khÃ¡c nhÆ°ng sample giá»‘ng)
  â†’ Mitigation: combine with file.size + file.lastModified
```

---

## TÃ³m Táº¯t

### Quick Reference

```
FILE UPLOAD â€” QUICK REF:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  CHUNKING:
  â†’ Blob.prototype.slice(start, end) â†’ chunk
  â†’ while (cur < file.size) â†’ push chunks[]
  â†’ FormData: chunk + hash(fileHash-index) + filename

  UPLOAD:
  â†’ Concurrent: Promise.all(chunks.map(upload))
  â†’ Merge: POST /merge â†’ readStream.pipe(writeStream, {start})
  â†’ sort chunks by index BEFORE merge!

  HASH:
  â†’ Web Worker + SparkMD5 (non-blocking!)
  â†’ FileReader.readAsArrayBuffer per chunk
  â†’ spark.append(each chunk) â†’ spark.end() = final hash

  INSTANT UPLOAD:
  â†’ POST /verify â†’ server check hash exists?
  â†’ YES â†’ {shouldUpload: false} â†’ "ç§’ä¼ !"
  â†’ NO  â†’ {shouldUpload: true, uploadedList: [...]}

  PAUSE:
  â†’ requestList[].forEach(xhr => xhr.abort())
  â†’ requestList tracks ACTIVE uploads only

  RESUME:
  â†’ POST /verify â†’ get uploadedList
  â†’ filter: skip uploaded â†’ upload remaining
  â†’ merge when uploaded + remaining = total

  PROGRESS:
  â†’ xhr.upload.onprogress per chunk
  â†’ Overall: Î£(size Ã— percentage) / totalSize
  â†’ Fake progress: only increase, never decrease (watch)

  PRODUCTION:
  â†’ Concurrency limit (3-6 parallel)
  â†’ Chunk retry on failure (exponential backoff)
  â†’ Hash sampling (first + middle + last)
  â†’ Server cleanup (TTL for orphan chunks)
```

### Checklist

- [ ] Blob.prototype.slice: file.slice(start, end) â†’ chunk Blob
- [ ] Chunk size: 5-10MB sweet spot (trade-off: requests vs retry cost)
- [ ] FormData: chunk(file) + hash(fileHash-index) + filename + fileHash
- [ ] Concurrent upload: Promise.all â†’ táº¥t cáº£ chunks song song
- [ ] Server: multiparty parse FormData â†’ save to chunkDir\_{fileHash}/
- [ ] Merge: readStream.pipe(writeStream, {start: index Ã— size})
- [ ] Sort chunks by index BEFORE merge (filesystem order â‰  upload order)
- [ ] XÃ³a chunk files SAU merge, xÃ³a chunkDir SAU merge hoÃ n táº¥t
- [ ] XHR.upload.onprogress: e.loaded / e.total Ã— 100 per chunk
- [ ] Overall progress: computed = Î£(size Ã— percentage) / file.size
- [ ] Content hash: Web Worker + SparkMD5 (KHÃ”NG block UI!)
- [ ] Worker: importScripts, postMessage, readAsArrayBuffer per chunk
- [ ] Instant upload: /verify â†’ check hash exists â†’ skip upload
- [ ] Pause: xhr.abort() on all requestList items
- [ ] Resume: /verify â†’ get uploadedList â†’ filter skip â†’ upload remaining
- [ ] Fake progress bar: watch real progress, only increase (never regress)
- [ ] Production: concurrency limit, retry, hash sampling, server cleanup
- [ ] Security: file type validation, size limit, rate limiting

---

_Nguá»“n: ByteDance Interview Question â€” Large File Upload & Resume_
_Cáº­p nháº­t láº§n cuá»‘i: ThÃ¡ng 2, 2026_
